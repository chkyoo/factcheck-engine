"""
ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ëª¨ë“ˆ (Option C)
ì£¼ìš” ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ì—¬ íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ìë™ ë°œê²¬
"""

import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime
from pathlib import Path
import sys

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from article_extractor import ArticleExtractor
from claim_detector import ClaimDetector
from priority_scorer import PriorityScorer


class NewsCrawler:
    """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"""
    
    # í¬ë¡¤ë§ ëŒ€ìƒ ì‚¬ì´íŠ¸
    TARGET_SITES = {
        'ë„¤ì´ë²„_ë‰´ìŠ¤_ì •ì¹˜': 'https://news.naver.com/section/100',
        'ë„¤ì´ë²„_ë‰´ìŠ¤_ê²½ì œ': 'https://news.naver.com/section/101',
    }
    
    def __init__(self, rate_limit=1.0):
        """
        Args:
            rate_limit: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.rate_limit = rate_limit
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        self.extractor = ArticleExtractor()
        self.detector = ClaimDetector()
        self.scorer = PriorityScorer()
    
    def crawl_all(self):
        """ëª¨ë“  ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        print("=" * 70)
        print(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 70)
        print()
        
        all_articles = []
        
        for site_name, site_url in self.TARGET_SITES.items():
            print(f"ğŸ•·ï¸  {site_name} í¬ë¡¤ë§ ì¤‘...")
            
            try:
                articles = self._crawl_naver_section(site_url)
                all_articles.extend(articles)
                print(f"  âœ“ {len(articles)}ê°œ ê¸°ì‚¬ URL ìˆ˜ì§‘")
                
            except Exception as e:
                print(f"  âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            
            # Rate limiting
            time.sleep(self.rate_limit)
            print()
        
        print("=" * 70)
        print(f"ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ URL ìˆ˜ì§‘ ì™„ë£Œ")
        print("=" * 70)
        print()
        
        return all_articles
    
    def _crawl_naver_section(self, url: str) -> list:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ í¬ë¡¤ë§"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°ì— ë§ê²Œ ì¡°ì • í•„ìš”)
            article_links = []
            
            # ì˜ˆì‹œ: a íƒœê·¸ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
            for link in soup.find_all('a', href=True):
                href = link['href']
                if 'news.naver.com' in href and '/article/' in href:
                    article_links.append(href)
            
            # ì¤‘ë³µ ì œê±°
            article_links = list(set(article_links))
            
            return article_links[:20]  # ìµœëŒ€ 20ê°œë§Œ
            
        except Exception as e:
            print(f"  âš ï¸  í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            return []
    
    def analyze_articles(self, article_urls: list, max_analyze=5):
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë¶„ì„"""
        print()
        print("ğŸ“Š ê¸°ì‚¬ ë¶„ì„ ì‹œì‘")
        print("-" * 70)
        print()
        
        high_priority_articles = []
        
        for i, url in enumerate(article_urls[:max_analyze], 1):
            print(f"[{i}/{min(max_analyze, len(article_urls))}] ë¶„ì„ ì¤‘...")
            
            try:
                # ê¸°ì‚¬ ì¶”ì¶œ
                article = self.extractor.extract(url)
                if not article:
                    print(f"  âŒ ì¶”ì¶œ ì‹¤íŒ¨: {url}")
                    continue
                
                # ì£¼ì¥ íƒì§€
                claims = self.detector.detect(article['text'])
                has_vague = self.detector.has_vague_source(article['text'])
                
                # ìš°ì„ ìˆœìœ„ ê³„ì‚°
                score_result = self.scorer.calculate_score(article, claims, has_vague)
                
                print(f"  âœ“ {article['title'][:40]}...")
                print(f"    ì ìˆ˜: {score_result['total_score']}ì  | ìš°ì„ ìˆœìœ„: {score_result['priority']}")
                
                if score_result['should_factcheck']:
                    high_priority_articles.append({
                        'url': url,
                        'article': article,
                        'claims': claims,
                        'score': score_result
                    })
                    print(f"    ğŸ¯ íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ!")
                
            except Exception as e:
                print(f"  âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # Rate limiting
            time.sleep(self.rate_limit)
            print()
        
        print("-" * 70)
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(high_priority_articles)}ê°œ íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ë°œê²¬")
        print()
        
        return high_priority_articles


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    crawler = NewsCrawler(rate_limit=1.0)
    
    # 1. í¬ë¡¤ë§
    article_urls = crawler.crawl_all()
    
    if not article_urls:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ë¶„ì„
    high_priority = crawler.analyze_articles(article_urls, max_analyze=5)
    
    # 3. ê²°ê³¼ ì¶œë ¥
    if high_priority:
        print()
        print("ğŸ“‹ íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ ëª©ë¡")
        print("=" * 70)
        
        for i, item in enumerate(high_priority, 1):
            article = item['article']
            score = item['score']
            
            print(f"\n[{i}] {article['title']}")
            print(f"    ì–¸ë¡ ì‚¬: {article['source']}")
            print(f"    ì ìˆ˜: {score['total_score']}ì  ({score['priority']})")
            print(f"    ì£¼ì¥ ìˆ˜: {score['claims_count']}ê°œ")
            print(f"    URL: {item['url']}")
        
        print()
        print("=" * 70)


if __name__ == "__main__":
    main()
